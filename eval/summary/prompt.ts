import { PromptTemplate } from '@langchain/core/prompts'

export const EVAL_SUMMARY_PROMPT = `
You are an expert researcher.
You have been given summaries of a research paper by different models and an original abstract.
You have to evaluate the summaries based on how well they capture the main points of the original abstract.

You will be given the following information:
- The original abstract of the research paper.
- The summaries generated by different models.

You will be asked to provide the following information:
- A score for each summary based on how well it captures the main points of the original abstract.


# Original Abstract

\`\`\`txt
{abstract}
\`\`\`

# Summaries

\`\`\`txt
{summaries}
\`\`\`


Please provide a score for each summary based on how well it captures the main points of the original abstract.
The response should be a JSON object with the key as the model name and the value as the score between 1 and 10.

[IMPORTANT] Only return the JSON and nothing else.
`

export const EVAL_PROMPT_TEMPLATE = PromptTemplate.fromTemplate(EVAL_SUMMARY_PROMPT)
